---
layout: post
title: "VGG2Vec: Extracting Image Representations from Neural Networks"
date: 2016-01-13 12:47:07
tags: project
---

In this post I will introduce VGG2Vec, a method to create a vector representation of images using neural networks. By extracting the vector representation of an image from a trained neural network, you can take advantage of what the neural network has learned about the picture. We'll visualize our representations to show that they encode information about different artists' styles as well as subject matter. 


### Neural Networks in 30 seconds

First, a quick primer on what neural networks are and how they work. Each layer of a neural network transforms its input data, warping the space it occupies. With multiple layers stacked atop each other, neural networks perform successive transformations to bring out relevant features. The final output is then used to represent the original input when it's fed into a (usually) logistic-regression classifier. 

![ Illustration of layer warping input space ]()

You don't really need to worry about most of the details of how neural networks work (if you want to know more, read [Chris Olah's superb explanations](http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/)). 

### Hierarchical representations

One of the neat things about deep networks is that because each layer takes its input from the previous layer, earlier layers highlight low-level features like lines and edges while later layers recognize composite features like shapes and objects. 

![ Illustration of hierarchical feature detection by layers ]()

The images above are illustrations of the features detected by each individual neuron in the first, second, and third layers of a neural network trained to recognize handwritten digits (MNIST). You can see that the features that the first layer is "looking for" tend to be simple patterns like curves and lines while the third layer activates on more complex patterns output by the previous layers. 


### The power of vector representations

Vector representations have been the target of tremendous interest over the past few years. If we can represent the data that interests us – text, images, audio – as a simple series of numbers (a vector), then they become much easier for computers to work with. Vector space is the range of values a vector can have; vectors with similar values are closer together in vector space. 

![ Illustration of word2vec relationships ](/assets/vgg2vec/word2vec.png)
*Vectors allow us to quantify and visualize semantic relationships (source: [Tensorflow tutorial](https://www.tensorflow.org/versions/master/tutorials/word2vec/index.html))*

[Word2Vec](https://www.tensorflow.org/versions/master/tutorials/word2vec/index.html) is a tool that Google released to construct vector representations of words based on their usage; words that have similar usages are mapped to nearby locations in vector space. Researchers quickly found that these vectors encoded all kinds of relationships, allowing for a mathematical understanding of the semantic space understood by the model. These vectors allow for operations like `king + woman = queen`


### VGG2Vec

VGG2Vec uses the [VGG-19 neural network]() (which has performed well on a variety of transfer-learning tasks) to construct a hierarchical representation of an input image. Specifically, by extracting the transformed input at selected layers, we can grab the features selected by the neural network without performing classification. 

What this gives us is a set of features from each layer of the neural network, encoding all of the patterns that the neural network has learned. In order to visualize some of these patterns, I applied t-SNE (a dimensionality reduction algorithm) to the VGG2Vec representations of a collection of Picasso paintings from different eras. As you can see below, the data grouped on content as well as style – themes that Picasso revisited in different styles are clustered together, demonstrating changes and refinements to specific ideas. 

I put together [an interactive demo](http://razi.xyz/vgg2vec/picasso) that you can browse around.

We can also use VGG2Vec to explore similarities and differences between different artists, as understood by VGG-19: [interactive demo](http://razi.xyz/vgg2vec/comparison)

### tl;dr:



VGG2Vec also presents a promising tool for art historians and digital humanists, who may seek to re-explore works through the eyes of a neural network. 
 

PS:

You might've noticed that I source two images from [Chris Olah's blog](http://colah.github.io/, which is phenomenal. His clear explanations were how I first started learning about neural networks and as I've continued to revisit his materials they continue to enlighten me in new ways. 

This post is a work in progress. To check out the VGG2Vec demos, look [here](http://razi.xyz/vgg2vec/picasso) and [here](http://razi.xyz/vgg2vec/comparison)

